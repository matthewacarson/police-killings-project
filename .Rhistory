df <- read_csv(paste0(data_dir, '\\data\\outputs\\databases\\zillow_database_2018.csv'))
# use this for Desktop R Studio
data_dir <- 'C:\\Users\\madou\\OneDrive - UCLA IT Services\\1)_PS-Honors\\police_killings_github\\udp_expansion_matt'
df <- read_csv(paste0(data_dir, '\\data\\outputs\\databases\\zillow_database_2018.csv'))
save(tr_rents, file = '~/data/R_data/tr_rents.Rdata'); gc()
save(tr_rents, file = '~\\data\\R_data\\tr_rents.Rdata'); gc()
save(tr_rents, file = paste(data_dir, '~/data/R_data/tr_rents.Rdata')); gc()
save(tr_rents, file = paste(data_dir, '/data/R_data/tr_rents.Rdata')); gc()
save(tr_rents, file = paste0(data_dir, '/data/R_data/tr_rents.Rdata')); gc()
save(tr_rents, file = paste0(data_dir, '\\data\\R_data\\tr_rents.Rdata')); gc()
save(df, file = paste0(data_dir, '\\data\\R_data\\zillow_database_2018.RData'))
# df <- read_csv(paste0(data_dir, '\\data\\outputs\\databases\\zillow_database_2018.csv'))
# save(df, file = paste0(data_dir, '\\data\\R_data\\zillow_database_2018.RData'))
load('zillow_database_2018.RData')
r_data_folder <- '\\data\\R_data\\'
# df <- read_csv(paste0(data_dir, '\\data\\outputs\\databases\\zillow_database_2018.csv'))
save(df, file = paste0(data_dir, r_data_folder, 'zillow_database_2018.RData'))
# use this for Desktop R Studio
data_dir <- 'C:\\Users\\madou\\OneDrive - UCLA IT Services\\1)_PS-Honors\\police_killings_github\\udp_expansion_matt'
r_data_folder <- '\\data\\R_data\\'
df <- read_csv(paste0(data_dir, '\\data\\outputs\\databases\\zillow_database_2018.csv'))
# df <- read_csv(paste0(data_dir, '\\data\\outputs\\databases\\zillow_database_2018.csv'))
save(df, file = paste0(data_dir, r_data_folder, 'zillow_database_2018.RData'))
# df <- read_csv(paste0(data_dir, '\\data\\outputs\\databases\\zillow_database_2018.csv'))
# save(df, file = paste0(data_dir, r_data_folder, 'zillow_database_2018.RData'))
load(paste0(data_dir, r_data_folder, 'zillow_database_2018.RData')
rm(df)
# df <- read_csv(paste0(data_dir, '\\data\\outputs\\databases\\zillow_database_2018.csv'))
# save(df, file = paste0(data_dir, r_data_folder, 'zillow_database_2018.RData'))
load(paste0(data_dir, r_data_folder, 'zillow_database_2018.RData'))
### Tract data extraction function: add your state here
st <- c("AL", "AK", "AZ", "AR", "CA", "CO", "CT",
"DE", "DC", "FL", "GA", "HI", "ID", "IL",
"IN", "IA", "KS", "KY", "LA", "ME", "MD",
"MA", "MI", "MN", "MS", "MO", "MT", "NE",
"NV", "NH", "NJ", "NM", "NY", "NC", "ND",
"OH", "OK", "OR", "PA", "RI", "SC", "SD",
"TN", "TX", "UT", "VT", "VA", "WA", "WV",
"WI", "WY")
tr_rent <- function(year, state){
get_acs(
geography = "tract",
variables = c('medrent' = 'B25064_001'),
state = state,
county = NULL,
geometry = FALSE,
cache_table = TRUE,
output = "tidy",
year = year,
keep_geo_vars = TRUE
) %>%
select(-moe) %>%
rename(medrent = estimate) %>%
mutate(
county = str_sub(GEOID, 3,5),
state = str_sub(GEOID, 1,2),
year = str_sub(year, 3,4)
)
}
### Loop (map) across different states
tr_rents18 <-
map_dfr(st, function(state){
tr_rent(year = 2018, state) %>%
mutate(COUNTY = substr(GEOID, 1, 5))
})
tr_rents12 <-
map_dfr(st, function(state){
tr_rent(year = 2012, state) %>%
mutate(
COUNTY = substr(GEOID, 1, 5),
medrent = medrent*1.07)
}); gc()
tr_rents <-
bind_rows(tr_rents18, tr_rents12) %>%
unite("variable", c(variable,year), sep = "") %>%
group_by(variable) %>%
spread(variable, medrent) %>%
group_by(COUNTY) %>%
mutate(
tr_medrent18 =
case_when(
is.na(medrent18) ~ median(medrent18, na.rm = TRUE),
TRUE ~ medrent18
),
tr_medrent12 =
case_when(
is.na(medrent12) ~ median(medrent12, na.rm = TRUE),
TRUE ~ medrent12),
tr_chrent = tr_medrent18 - tr_medrent12,
tr_pchrent = (tr_medrent18 - tr_medrent12)/tr_medrent12,
rm_medrent18 = median(tr_medrent18, na.rm = TRUE),
rm_medrent12 = median(tr_medrent12, na.rm = TRUE)) %>%
select(-medrent12, -medrent18) %>%
distinct() %>%
group_by(GEOID) %>%
filter(row_number() == 1) %>%
ungroup()
rm(tr_rents12, tr_rents18)
save(tr_rents, file = paste0(data_dir, r_data_folder, 'tr_rents.Rdata')); gc()
load("C:/Users/madou/OneDrive - UCLA IT Services/1)_PS-Honors/police_killings_github/udp_expansion_matt/data/R_data/sp_sates_1_thru_47.RData")
load("C:/Users/madou/OneDrive - UCLA IT Services/1)_PS-Honors/police_killings_github/udp_expansion_matt/data/R_data/sp_sates.RData")
reticulate::repl_python()
import pandas as pd
from shapely import wkt
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from shapely import wkt
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import sys
import matplotlib.pyplot as plt
home = str(Path.home())
py_save_session("mysession")
wd()
syswd()
os.getcwd
os.getcwd()
import os
os.getcwd
os.getcwd()
Path.home()
home = os.getcwd()
input_path = home+'data/inputs/'
home+input_path
input_path = home+'\\data\\inputs\\'
home+input_path
output_path = home+'\\data\\outputs\\'
home = os.getcwd()
os.chdir(home)
input_path = home+'\\data\\inputs\\'
output_path = home+'\\data\\outputs\\'
home + output_path+ '~/git/displacement-typologies/data/outputs/lags/lag.csv'
home + output_path + 'lags/lag.csv'
home + output_path + 'lags\\lag.csv'
home + output_path + 'lags\\lag.csv'
os.getcwd()
input_path
home = os.getcwd() + '\\udp_expansion_matt\\'
os.chdir(home)
os.getcwd()
os.chdir(home)
input_path = home + '\\data\\inputs\\'
output_path = home + '\\data\\outputs\\'
output_path
output_path
typology_input = pd.read_csv(output_path+ 'databases\\zillow_database_2018.csv', index_col = 0) ### Read file
lag = pd.read_csv(output_path + 'lags\\lag.csv')
lag = pd.read_csv(output_path + 'lags\\lag.csv')
View(lag)
View(typology_input)
typology_input['geometry'] = typology_input['geometry'].apply(wkt.loads) ### Read geometry as a shp attribute
typology_input.columns
9*9*9*9*9*9
5156156*5115
551**2
551**5
551**9
View(typology_input)
typology_input['geometry']
print(typology_input.columns)
str(typology_input.columns)
print(typology_input.columns.to_list())
column_names = typology_input.columns.to_list()
# Specify the file path where you want to save the .txt file
file_path = "column_names.txt"
# Open the file in write mode
with open(file_path, "w") as file:
# Write each column name to the file
for name in column_names:
file.write(name + "\n")
census_90 = pd.read_csv('R13437364_SL140_1990.csv') #, dtype=dtype_dict)
# census_90 = pd.read_csv('R13437364_SL140_subset.csv', index_col = 0)
# dtype_dict = {col: float for col in range(1, 51)}
census_00 = pd.read_csv('merged_2000.csv')#, dtype=dtype_dict)
# Crosswalk files
# dtype_dict = {col: float for col in range(0, 51)}
xwalk_90_10 = pd.read_csv(input_path+'crosswalk_1990_2010.csv')#, dtype=dtype_dict)
xwalk_00_10 = pd.read_csv(input_path+'crosswalk_2000_2010.csv')#, dtype=dtype_dict)
# ####
# =====================================================
# Create Crosswalk Functions / Files
# =====================================================
#%% 3
def crosswalk_files (df, xwalk, counts, medians, df_fips_base, xwalk_fips_base, xwalk_fips_horizon):
# merge dataframe with xwalk file
df_merge = df.merge(xwalk[['weight', xwalk_fips_base, xwalk_fips_horizon]], left_on = df_fips_base, right_on = xwalk_fips_base, how='left')
df = df_merge
# apply interpolation weight
new_var_list = list(counts)+(medians)
for var in new_var_list:
df[var] = df[var]*df['weight']
# aggregate by horizon census tracts fips
df = df.groupby(xwalk_fips_horizon).sum().reset_index()
# rename trtid10 to FIPS & FIPS to trtid_base
df = df.rename(columns = {'FIPS':'trtid_base',
'trtid10':'FIPS'})
# fix state, county and fips code
df ['state'] = df['FIPS'].astype('int64').astype(str).str.zfill(11).str[0:2]
df ['county'] = df['FIPS'].astype('int64').astype(str).str.zfill(11).str[2:5]
df ['tract'] = df['FIPS'].astype('int64').astype(str).str.zfill(11).str[5:]
# drop weight column
df = df.drop(columns = ['weight'])
return df
#%% 4
# Crosswalking
# --------------------------------------------------
# 9/11: I am running into some problems with multiplying non-integers (see error message below)
# TypeError: can't multiply sequence by non-int of type 'float'
# Use .dtypes to check the data storage types
# Check lists with type(list_name)
## 1990 Census Data
counts = census_90.columns.drop(['mrent_90', 'mhval_90', 'hinc_90', "NAME", "QName", "SUMLEV", "GEOCOMP", "REGION", "DIVISION", "FIPS", "state", "county", "tract"])
medians = ['mrent_90', 'mhval_90', 'hinc_90']
df_fips_base = 'FIPS'
xwalk_fips_base = 'trtid90'
xwalk_fips_horizon = 'trtid10'
census_90_xwalked = crosswalk_files (census_90, xwalk_90_10,  counts, medians, df_fips_base, xwalk_fips_base, xwalk_fips_horizon )
del [census_90, xwalk_90_10]
#%% 5
## 2000 Census Data
counts = census_00.columns.drop(['mrent_00', 'mhval_00', 'hinc_00', 'NAME', 'FIPS'])
medians = ['mrent_00', 'mhval_00', 'hinc_00']
df_fips_base = 'FIPS'
xwalk_fips_base = 'trtid00'
xwalk_fips_horizon = 'trtid10'
census_00_xwalked = crosswalk_files (census_00, xwalk_00_10,  counts, medians, df_fips_base, xwalk_fips_base, xwalk_fips_horizon )
del [census_00, xwalk_00_10]
#%% 6
# =====================================================
# Variable Creation
# =====================================================
# =====================================================
# Setup / Read Files (inputs needed)
# =====================================================
# Note: Below is the Google File Drive Stream pathway for a Mac.
# input_path = '~/git/displacement-typologies/data/inputs/'
# Use this to draw in the 'input_path' variable needed below
# You will need to redesignate this path if you have a Windows
# output_path = output_path
# ################################################################ #
# I named the merged 2012 and 2018 acs files to merged_2012_2018.csv
# The data below was downloaded using matts_download_tidycensus.R
# ################################################################ #
census_2012_2018 = pd.read_csv('merged_2012_2018.csv')#, index_col = 0)
# census_2012_2018 = census_2012_2018.drop(columns = ['county_y', 'state_y', 'tract_y'])
#%% 7
census_2012_2018 = census_2012_2018.rename(columns = {'county_x': 'county',
'state_x': 'state',
'tract_x': 'tract',
'GEOID': 'FIPS'}) # 9/11/23: I added this because tidycensus downloaded as GEOID
#%% 8
# Bring in PUMS data
# =====================================================
pums_r = pd.read_csv(input_path+'nhgis0002_ds233_20175_2017_tract.csv', encoding = "ISO-8859-1")
pums_o = pd.read_csv(input_path+'nhgis0002_ds234_20175_2017_tract.csv', encoding = "ISO-8859-1")
pums = pums_r.merge(pums_o, on = 'GISJOIN')
pums = pums.rename(columns = {'YEAR_x':'YEAR',
'STATE_x':'STATE',
'STATEA_x':'STATEA',
'COUNTY_x':'COUNTY',
'COUNTYA_x':'COUNTYA',
'TRACTA_x':'TRACTA',
'NAME_E_x':'NAME_E'})
pums = pums.dropna(axis = 1)
del [pums_r, pums_o]
#%% 9
## Zillow data
zillow = pd.read_csv(input_path+'Zip_Zhvi_AllHomes.csv', encoding = "ISO-8859-1")
zillow_xwalk = pd.read_csv(input_path+'TRACT_ZIP_032015.csv')
lihtc = pd.read_csv(input_path+'LowIncome_Housing_Tax_Credit_Properties.csv', na_values=[''])
rail = pd.read_csv(input_path+'tod_database_download.csv')
hospitals = pd.read_csv(input_path+'Hospitals.csv')
university = pd.read_csv(input_path+'university_HD2016.csv')
pub_hous = pd.read_csv(input_path+'Public_Housing_Buildings.csv.gz')
census = census_2012_2018.merge(census_00_xwalked, on = 'FIPS', how = 'outer').merge(census_90_xwalked, on = 'FIPS', how = 'outer')
CPI_89_18 = 2.08
CPI_99_18 = 1.53
CPI_12_18 = 1.11
## This is used for the Zillow data, where january values are compared
CPI_0115_0119 = 1.077
exit
for (i in 1:12)
print(i)
for (i in 5:12) print(i)
for (i in 5:90) print(i)
for (i in 5:900) print(i)
for (i in 5:90) i
for (i in 5:90) return(i)
for (i in 5:90) print(i)
for (number in 5:90) print(number)
for (number in 5:15) print(number)
for (number in 5:35) print(number)
paste0(data_dir, r_data_folder, '3_lag_vars_everything')
data_dir = 'matt/dsfsdf/sdf'
r_data_folder = '/R_data'
paste0(data_dir, r_data_folder, '3_lag_vars_everything')
reticulate::repl_python()
quit
knitr::opts_chunk$set(echo = TRUE)
reticulate::repl_python()
import pandas as pd
mylist = [1, 4, 7, 12]
mylist * 2
quit
plot(pressure)
reticulate::repl_python()
mylist * 2
quit
summary(cars)
reticulate::repl_python()
mylist = [1, 4, 7, 12]
quit
# install from CRAN
install.packages(c('repr', 'IRdisplay',
'evaluate', 'crayon',
'pbdZMQ', 'devtools',
'uuid', 'digest'))
# from Github
devtools::install_github('IRkernel/IRkernel')
install.packages(c("repr", "IRdisplay", "evaluate", "crayon", "pbdZMQ", "devtools", "uuid", "digest"))
install.packages(c("repr", "IRdisplay", "evaluate", "crayon", "pbdZMQ", "devtools", "uuid", "digest"))
install.packages(c("repr", "IRdisplay", "evaluate", "crayon", "pbdZMQ", "devtools", "uuid", "digest"))
install.packages(c("repr", "IRdisplay", "evaluate", "crayon", "pbdZMQ", "devtools", "uuid", "digest"))
install.packages('digest')
install.packages("digest")
install.packages("digest")
if (!require(pacman)) install.packages("pacman"); pacman::p_load(googledrive, bit64, fs, data.table, tigris, tidycensus, tidyverse, spdep)
# use this for Desktop R Studio
data_dir <- 'C:\\Users\\madou\\OneDrive - UCLA IT Services\\1)_PS-Honors\\police_killings_github\\udp_expansion_matt'
r_data_folder <- '\\data\\R_data\\'
# df <- read_csv(paste0(data_dir, '\\data\\outputs\\databases\\zillow_database_2018.csv'))
# save(df, file = paste0(data_dir, r_data_folder, 'zillow_database_2018.RData'))
load(paste0(data_dir, r_data_folder, 'zillow_database_2018.RData'))
### Tract data extraction function: add your state here
st <- c("AL", "AK", "AZ", "AR", "CA", "CO", "CT",
"DE", "DC", "FL", "GA", "HI", "ID", "IL",
"IN", "IA", "KS", "KY", "LA", "ME", "MD",
"MA", "MI", "MN", "MS", "MO", "MT", "NE",
"NV", "NH", "NJ", "NM", "NY", "NC", "ND",
"OH", "OK", "OR", "PA", "RI", "SC", "SD",
"TN", "TX", "UT", "VT", "VA", "WA", "WV",
"WI", "WY")
# use this for Desktop R Studio
# data_dir <- 'C:/Users/madou/OneDrive - UCLA IT Services/1)_PS-Honors/police_killings_github/udp_expansion_matt'
data_dir <- paste0(getwd(), '/udp_expansion_matt')
r_data_folder <- '/data/R_data/'
### Tract data extraction function: add your state here
st <- c("AL", "AK", "AZ", "AR", "CA", "CO", "CT",
"DE", "DC", "FL", "GA", "HI", "ID", "IL",
"IN", "IA", "KS", "KY", "LA", "ME", "MD",
"MA", "MI", "MN", "MS", "MO", "MT", "NE",
"NV", "NH", "NJ", "NM", "NY", "NC", "ND",
"OH", "OK", "OR", "PA", "RI", "SC", "SD",
"TN", "TX", "UT", "VT", "VA", "WA", "WV",
"WI", "WY")
#         tr_pchrent = (tr_medrent18 - tr_medrent12)/tr_medrent12,
#         rm_medrent18 = median(tr_medrent18, na.rm = TRUE),
#         rm_medrent12 = median(tr_medrent12, na.rm = TRUE)) %>%
#     select(-medrent12, -medrent18) %>%
#     distinct() %>%
#     group_by(GEOID) %>%
#     filter(row_number() == 1) %>%
#     ungroup()
# rm(tr_rents12, tr_rents18)
# save(tr_rents, file = paste0(data_dir, r_data_folder, 'tr_rents.Rdata'))
load(file = paste0(data_dir, r_data_folder, 'tr_rents.Rdata'))
stsp <- combined_tracts; rm(combined_tracts)
load("C:/Users/madou/OneDrive - UCLA IT Services/1)_PS-Honors/police_killings_github/udp_expansion_matt/data/R_data/st_thru_51.RData")
load(file = paste0(data_dir, r_data_folder, 'st_thru_51.RData'))
stsp <- combined_tracts; rm(combined_tracts)
# load(file = paste0(data_dir, r_data_folder, 'states_final.RData'))
# debug(left_join)
# undebug(left_join)
# join data to these tracts
stsp@data <-
left_join(
stsp@data,
tr_rents,
by = "GEOID") # %>%
stsp@data
view(st_thru_51.RData)
view(stsp@data)
# load(file = paste0(data_dir, r_data_folder, 'states_final.RData'))
# debug(left_join)
# undebug(left_join)
# join data to these tracts
stsp@data <-
left_join(
stsp@data,
tr_rents,
by = "GEOID") %>%
select(5:23)
# load(file = paste0(data_dir, r_data_folder, 'states_final.RData'))
# debug(left_join)
# undebug(left_join)
# join data to these tracts
stsp@data <-
left_join(
stsp@data,
tr_rents,
by = "GEOID") %>% select(2)
View(stsp)
# load(file = paste0(data_dir, r_data_folder, 'states_final.RData'))
# debug(left_join)
# undebug(left_join)
# join data to these tracts
stsp@data <-
left_join(
stsp@data,
tr_rents,
by = "GEOID") %>% select(5:23)
load(file = paste0(data_dir, r_data_folder, 'st_thru_51.RData'))
stsp <- combined_tracts; rm(combined_tracts)
# load(file = paste0(data_dir, r_data_folder, 'states_final.RData'))
# debug(left_join)
# undebug(left_join)
# join data to these tracts
stsp@data <-
left_join(
stsp@data,
tr_rents,
by = "GEOID") %>% select(5:23)
View(stsp)
view(stsp@data)
ncol(st_thru_51.RData)
ncol(stsp@data)
View(stsp@data)
#
# Create neighbor matrix
# -----------------------------------------------------
coords <- coordinates(stsp)
if (!require(pacman)) install.packages("pacman"); pacman::p_load(googledrive, bit64, fs, data.table, tigris, tidycensus, tidyverse, spdep)
# use this for Desktop R Studio
# data_dir <- 'C:/Users/madou/OneDrive - UCLA IT Services/1)_PS-Honors/police_killings_github/udp_expansion_matt'
data_dir <- paste0(getwd(), '/udp_expansion_matt')
r_data_folder <- '/data/R_data/'
### Tract data extraction function: add your state here
st <- c("AL", "AK", "AZ", "AR", "CA", "CO", "CT",
"DE", "DC", "FL", "GA", "HI", "ID", "IL",
"IN", "IA", "KS", "KY", "LA", "ME", "MD",
"MA", "MI", "MN", "MS", "MO", "MT", "NE",
"NV", "NH", "NJ", "NM", "NY", "NC", "ND",
"OH", "OK", "OR", "PA", "RI", "SC", "SD",
"TN", "TX", "UT", "VT", "VA", "WA", "WV",
"WI", "WY")
#         tr_pchrent = (tr_medrent18 - tr_medrent12)/tr_medrent12,
#         rm_medrent18 = median(tr_medrent18, na.rm = TRUE),
#         rm_medrent12 = median(tr_medrent12, na.rm = TRUE)) %>%
#     select(-medrent12, -medrent18) %>%
#     distinct() %>%
#     group_by(GEOID) %>%
#     filter(row_number() == 1) %>%
#     ungroup()
# rm(tr_rents12, tr_rents18)
# save(tr_rents, file = paste0(data_dir, r_data_folder, 'tr_rents.Rdata'))
load(file = paste0(data_dir, r_data_folder, 'tr_rents.Rdata'))
#
# Create neighbor matrix
# -----------------------------------------------------
coords <- coordinates(stsp)
if (!require(pacman)) install.packages("pacman"); pacman::p_load(googledrive, bit64, fs, data.table, tigris, tidycensus, tidyverse, spdep, raster, sp, parallel, sf)
#
# Create neighbor matrix
# -----------------------------------------------------
coords <- coordinates(stsp)
IDs <- row.names(as(stsp, "data.frame"))
stsp_nb <- poly2nb(stsp) # nb
lw_bin <- nb2listw(stsp_nb, style = "W", zero.policy = TRUE)
kern1 <- knn2nb(knearneigh(coords, k = 1), row.names=IDs)
detectCores()
# Load necessary packages
library(foreach)
library(doParallel)
library(doParallel)
# Set up parallel backend
# cores <- detectCores()
cl <- makeCluster(4)
registerDoParallel(cl)
# Return the results you need
# return(list(lw_bin = lw_bin, kern1 = kern1))
}
# Function to parallelize
parallel_function <- function() {
dist <<- unlist(nbdists(kern1, coords)); summary(dist)
max_1nn <<- max(dist)
dist_nb <<- dnearneigh(coords, d1=0, d2 = .1*max_1nn, row.names = IDs)
# Return the results you need
# return(list(lw_bin = lw_bin, kern1 = kern1))
}
# Function to parallelize
parallel_function <- function() {
dist <<- unlist(nbdists(kern1, coords)); summary(dist)
max_1nn <<- max(dist)
dist_nb <<- dnearneigh(coords, d1=0, d2 = .1*max_1nn, row.names = IDs)
# Return the results you need
# return(list(lw_bin = lw_bin, kern1 = kern1))
}
# Run the function in parallel
parallel_function()
dist <- unlist(nbdists(kern1, coords)); summary(dist)
max_1nn <- max(dist)
dist_nb <- dnearneigh(coords, d1=0, d2 = .1*max_1nn, row.names = IDs)
spdep::set.ZeroPolicyOption(TRUE)
spdep::set.ZeroPolicyOption(TRUE)
dists <<- nbdists(dist_nb, coordinates(stsp))
idw <<- lapply(dists, function(x) 1/(x^2))
lw_dist_idwW <<- nb2listw(dist_nb, glist = idw, style = "W")
save.image("C:/Users/madou/OneDrive - UCLA IT Services/1)_PS-Honors/police_killings_github/Feb_13_2024_1_00_PM.RData")
