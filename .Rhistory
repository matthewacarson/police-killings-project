TRUE ~ medrent12),
tr_chrent = tr_medrent18 - tr_medrent12,
tr_pchrent = (tr_medrent18 - tr_medrent12)/tr_medrent12,
rm_medrent18 = median(tr_medrent18, na.rm = TRUE),
rm_medrent12 = median(tr_medrent12, na.rm = TRUE)) %>%
select(-medrent12, -medrent18) %>%
distinct() %>%
group_by(GEOID) %>%
filter(row_number() == 1) %>%
ungroup()
save(tr_rents, file = 'tr_rents.Rdata')
rm(tr_rents12, tr_rents18)
setwd("C:/Users/madou/OneDrive - UCLA IT Services/1)_PS-Honors/police_killings_github/udp_expansion_matt/data/R_data")
setwd("C:/Users/madou/OneDrive - UCLA IT Services/1)_PS-Honors/police_killings_github")
save(tr_rents, file = '~/udp_expansion_matt/data/R_data/tr_rents.Rdata'); gc()
getwd()
df <- read_csv(paste0(data_dir, '\\data\\outputs\\databases\\zillow_database_2018.csv'))
# use this for Desktop R Studio
data_dir <- 'C:\\Users\\madou\\OneDrive - UCLA IT Services\\1)_PS-Honors\\police_killings_github\\udp_expansion_matt'
df <- read_csv(paste0(data_dir, '\\data\\outputs\\databases\\zillow_database_2018.csv'))
save(tr_rents, file = '~/data/R_data/tr_rents.Rdata'); gc()
save(tr_rents, file = '~\\data\\R_data\\tr_rents.Rdata'); gc()
save(tr_rents, file = paste(data_dir, '~/data/R_data/tr_rents.Rdata')); gc()
save(tr_rents, file = paste(data_dir, '/data/R_data/tr_rents.Rdata')); gc()
save(tr_rents, file = paste0(data_dir, '/data/R_data/tr_rents.Rdata')); gc()
save(tr_rents, file = paste0(data_dir, '\\data\\R_data\\tr_rents.Rdata')); gc()
save(df, file = paste0(data_dir, '\\data\\R_data\\zillow_database_2018.RData'))
# df <- read_csv(paste0(data_dir, '\\data\\outputs\\databases\\zillow_database_2018.csv'))
# save(df, file = paste0(data_dir, '\\data\\R_data\\zillow_database_2018.RData'))
load('zillow_database_2018.RData')
r_data_folder <- '\\data\\R_data\\'
# df <- read_csv(paste0(data_dir, '\\data\\outputs\\databases\\zillow_database_2018.csv'))
save(df, file = paste0(data_dir, r_data_folder, 'zillow_database_2018.RData'))
# use this for Desktop R Studio
data_dir <- 'C:\\Users\\madou\\OneDrive - UCLA IT Services\\1)_PS-Honors\\police_killings_github\\udp_expansion_matt'
r_data_folder <- '\\data\\R_data\\'
df <- read_csv(paste0(data_dir, '\\data\\outputs\\databases\\zillow_database_2018.csv'))
# df <- read_csv(paste0(data_dir, '\\data\\outputs\\databases\\zillow_database_2018.csv'))
save(df, file = paste0(data_dir, r_data_folder, 'zillow_database_2018.RData'))
# df <- read_csv(paste0(data_dir, '\\data\\outputs\\databases\\zillow_database_2018.csv'))
# save(df, file = paste0(data_dir, r_data_folder, 'zillow_database_2018.RData'))
load(paste0(data_dir, r_data_folder, 'zillow_database_2018.RData')
rm(df)
# df <- read_csv(paste0(data_dir, '\\data\\outputs\\databases\\zillow_database_2018.csv'))
# save(df, file = paste0(data_dir, r_data_folder, 'zillow_database_2018.RData'))
load(paste0(data_dir, r_data_folder, 'zillow_database_2018.RData'))
### Tract data extraction function: add your state here
st <- c("AL", "AK", "AZ", "AR", "CA", "CO", "CT",
"DE", "DC", "FL", "GA", "HI", "ID", "IL",
"IN", "IA", "KS", "KY", "LA", "ME", "MD",
"MA", "MI", "MN", "MS", "MO", "MT", "NE",
"NV", "NH", "NJ", "NM", "NY", "NC", "ND",
"OH", "OK", "OR", "PA", "RI", "SC", "SD",
"TN", "TX", "UT", "VT", "VA", "WA", "WV",
"WI", "WY")
tr_rent <- function(year, state){
get_acs(
geography = "tract",
variables = c('medrent' = 'B25064_001'),
state = state,
county = NULL,
geometry = FALSE,
cache_table = TRUE,
output = "tidy",
year = year,
keep_geo_vars = TRUE
) %>%
select(-moe) %>%
rename(medrent = estimate) %>%
mutate(
county = str_sub(GEOID, 3,5),
state = str_sub(GEOID, 1,2),
year = str_sub(year, 3,4)
)
}
### Loop (map) across different states
tr_rents18 <-
map_dfr(st, function(state){
tr_rent(year = 2018, state) %>%
mutate(COUNTY = substr(GEOID, 1, 5))
})
tr_rents12 <-
map_dfr(st, function(state){
tr_rent(year = 2012, state) %>%
mutate(
COUNTY = substr(GEOID, 1, 5),
medrent = medrent*1.07)
}); gc()
tr_rents <-
bind_rows(tr_rents18, tr_rents12) %>%
unite("variable", c(variable,year), sep = "") %>%
group_by(variable) %>%
spread(variable, medrent) %>%
group_by(COUNTY) %>%
mutate(
tr_medrent18 =
case_when(
is.na(medrent18) ~ median(medrent18, na.rm = TRUE),
TRUE ~ medrent18
),
tr_medrent12 =
case_when(
is.na(medrent12) ~ median(medrent12, na.rm = TRUE),
TRUE ~ medrent12),
tr_chrent = tr_medrent18 - tr_medrent12,
tr_pchrent = (tr_medrent18 - tr_medrent12)/tr_medrent12,
rm_medrent18 = median(tr_medrent18, na.rm = TRUE),
rm_medrent12 = median(tr_medrent12, na.rm = TRUE)) %>%
select(-medrent12, -medrent18) %>%
distinct() %>%
group_by(GEOID) %>%
filter(row_number() == 1) %>%
ungroup()
rm(tr_rents12, tr_rents18)
save(tr_rents, file = paste0(data_dir, r_data_folder, 'tr_rents.Rdata')); gc()
load("C:/Users/madou/OneDrive - UCLA IT Services/1)_PS-Honors/police_killings_github/udp_expansion_matt/data/R_data/sp_sates_1_thru_47.RData")
load("C:/Users/madou/OneDrive - UCLA IT Services/1)_PS-Honors/police_killings_github/udp_expansion_matt/data/R_data/sp_sates.RData")
reticulate::repl_python()
import pandas as pd
from shapely import wkt
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from shapely import wkt
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import sys
import matplotlib.pyplot as plt
home = str(Path.home())
py_save_session("mysession")
wd()
syswd()
os.getcwd
os.getcwd()
import os
os.getcwd
os.getcwd()
Path.home()
home = os.getcwd()
input_path = home+'data/inputs/'
home+input_path
input_path = home+'\\data\\inputs\\'
home+input_path
output_path = home+'\\data\\outputs\\'
home = os.getcwd()
os.chdir(home)
input_path = home+'\\data\\inputs\\'
output_path = home+'\\data\\outputs\\'
home + output_path+ '~/git/displacement-typologies/data/outputs/lags/lag.csv'
home + output_path + 'lags/lag.csv'
home + output_path + 'lags\\lag.csv'
home + output_path + 'lags\\lag.csv'
os.getcwd()
input_path
home = os.getcwd() + '\\udp_expansion_matt\\'
os.chdir(home)
os.getcwd()
os.chdir(home)
input_path = home + '\\data\\inputs\\'
output_path = home + '\\data\\outputs\\'
output_path
output_path
typology_input = pd.read_csv(output_path+ 'databases\\zillow_database_2018.csv', index_col = 0) ### Read file
lag = pd.read_csv(output_path + 'lags\\lag.csv')
lag = pd.read_csv(output_path + 'lags\\lag.csv')
View(lag)
View(typology_input)
typology_input['geometry'] = typology_input['geometry'].apply(wkt.loads) ### Read geometry as a shp attribute
typology_input.columns
9*9*9*9*9*9
5156156*5115
551**2
551**5
551**9
View(typology_input)
typology_input['geometry']
print(typology_input.columns)
str(typology_input.columns)
print(typology_input.columns.to_list())
column_names = typology_input.columns.to_list()
# Specify the file path where you want to save the .txt file
file_path = "column_names.txt"
# Open the file in write mode
with open(file_path, "w") as file:
# Write each column name to the file
for name in column_names:
file.write(name + "\n")
census_90 = pd.read_csv('R13437364_SL140_1990.csv') #, dtype=dtype_dict)
# census_90 = pd.read_csv('R13437364_SL140_subset.csv', index_col = 0)
# dtype_dict = {col: float for col in range(1, 51)}
census_00 = pd.read_csv('merged_2000.csv')#, dtype=dtype_dict)
# Crosswalk files
# dtype_dict = {col: float for col in range(0, 51)}
xwalk_90_10 = pd.read_csv(input_path+'crosswalk_1990_2010.csv')#, dtype=dtype_dict)
xwalk_00_10 = pd.read_csv(input_path+'crosswalk_2000_2010.csv')#, dtype=dtype_dict)
# ####
# =====================================================
# Create Crosswalk Functions / Files
# =====================================================
#%% 3
def crosswalk_files (df, xwalk, counts, medians, df_fips_base, xwalk_fips_base, xwalk_fips_horizon):
# merge dataframe with xwalk file
df_merge = df.merge(xwalk[['weight', xwalk_fips_base, xwalk_fips_horizon]], left_on = df_fips_base, right_on = xwalk_fips_base, how='left')
df = df_merge
# apply interpolation weight
new_var_list = list(counts)+(medians)
for var in new_var_list:
df[var] = df[var]*df['weight']
# aggregate by horizon census tracts fips
df = df.groupby(xwalk_fips_horizon).sum().reset_index()
# rename trtid10 to FIPS & FIPS to trtid_base
df = df.rename(columns = {'FIPS':'trtid_base',
'trtid10':'FIPS'})
# fix state, county and fips code
df ['state'] = df['FIPS'].astype('int64').astype(str).str.zfill(11).str[0:2]
df ['county'] = df['FIPS'].astype('int64').astype(str).str.zfill(11).str[2:5]
df ['tract'] = df['FIPS'].astype('int64').astype(str).str.zfill(11).str[5:]
# drop weight column
df = df.drop(columns = ['weight'])
return df
#%% 4
# Crosswalking
# --------------------------------------------------
# 9/11: I am running into some problems with multiplying non-integers (see error message below)
# TypeError: can't multiply sequence by non-int of type 'float'
# Use .dtypes to check the data storage types
# Check lists with type(list_name)
## 1990 Census Data
counts = census_90.columns.drop(['mrent_90', 'mhval_90', 'hinc_90', "NAME", "QName", "SUMLEV", "GEOCOMP", "REGION", "DIVISION", "FIPS", "state", "county", "tract"])
medians = ['mrent_90', 'mhval_90', 'hinc_90']
df_fips_base = 'FIPS'
xwalk_fips_base = 'trtid90'
xwalk_fips_horizon = 'trtid10'
census_90_xwalked = crosswalk_files (census_90, xwalk_90_10,  counts, medians, df_fips_base, xwalk_fips_base, xwalk_fips_horizon )
del [census_90, xwalk_90_10]
#%% 5
## 2000 Census Data
counts = census_00.columns.drop(['mrent_00', 'mhval_00', 'hinc_00', 'NAME', 'FIPS'])
medians = ['mrent_00', 'mhval_00', 'hinc_00']
df_fips_base = 'FIPS'
xwalk_fips_base = 'trtid00'
xwalk_fips_horizon = 'trtid10'
census_00_xwalked = crosswalk_files (census_00, xwalk_00_10,  counts, medians, df_fips_base, xwalk_fips_base, xwalk_fips_horizon )
del [census_00, xwalk_00_10]
#%% 6
# =====================================================
# Variable Creation
# =====================================================
# =====================================================
# Setup / Read Files (inputs needed)
# =====================================================
# Note: Below is the Google File Drive Stream pathway for a Mac.
# input_path = '~/git/displacement-typologies/data/inputs/'
# Use this to draw in the 'input_path' variable needed below
# You will need to redesignate this path if you have a Windows
# output_path = output_path
# ################################################################ #
# I named the merged 2012 and 2018 acs files to merged_2012_2018.csv
# The data below was downloaded using matts_download_tidycensus.R
# ################################################################ #
census_2012_2018 = pd.read_csv('merged_2012_2018.csv')#, index_col = 0)
# census_2012_2018 = census_2012_2018.drop(columns = ['county_y', 'state_y', 'tract_y'])
#%% 7
census_2012_2018 = census_2012_2018.rename(columns = {'county_x': 'county',
'state_x': 'state',
'tract_x': 'tract',
'GEOID': 'FIPS'}) # 9/11/23: I added this because tidycensus downloaded as GEOID
#%% 8
# Bring in PUMS data
# =====================================================
pums_r = pd.read_csv(input_path+'nhgis0002_ds233_20175_2017_tract.csv', encoding = "ISO-8859-1")
pums_o = pd.read_csv(input_path+'nhgis0002_ds234_20175_2017_tract.csv', encoding = "ISO-8859-1")
pums = pums_r.merge(pums_o, on = 'GISJOIN')
pums = pums.rename(columns = {'YEAR_x':'YEAR',
'STATE_x':'STATE',
'STATEA_x':'STATEA',
'COUNTY_x':'COUNTY',
'COUNTYA_x':'COUNTYA',
'TRACTA_x':'TRACTA',
'NAME_E_x':'NAME_E'})
pums = pums.dropna(axis = 1)
del [pums_r, pums_o]
#%% 9
## Zillow data
zillow = pd.read_csv(input_path+'Zip_Zhvi_AllHomes.csv', encoding = "ISO-8859-1")
zillow_xwalk = pd.read_csv(input_path+'TRACT_ZIP_032015.csv')
lihtc = pd.read_csv(input_path+'LowIncome_Housing_Tax_Credit_Properties.csv', na_values=[''])
rail = pd.read_csv(input_path+'tod_database_download.csv')
hospitals = pd.read_csv(input_path+'Hospitals.csv')
university = pd.read_csv(input_path+'university_HD2016.csv')
pub_hous = pd.read_csv(input_path+'Public_Housing_Buildings.csv.gz')
census = census_2012_2018.merge(census_00_xwalked, on = 'FIPS', how = 'outer').merge(census_90_xwalked, on = 'FIPS', how = 'outer')
CPI_89_18 = 2.08
CPI_99_18 = 1.53
CPI_12_18 = 1.11
## This is used for the Zillow data, where january values are compared
CPI_0115_0119 = 1.077
exit
for (i in 1:12)
print(i)
for (i in 5:12) print(i)
for (i in 5:90) print(i)
for (i in 5:900) print(i)
for (i in 5:90) i
for (i in 5:90) return(i)
for (i in 5:90) print(i)
for (number in 5:90) print(number)
for (number in 5:15) print(number)
for (number in 5:35) print(number)
paste0(data_dir, r_data_folder, '3_lag_vars_everything')
data_dir = 'matt/dsfsdf/sdf'
r_data_folder = '/R_data'
paste0(data_dir, r_data_folder, '3_lag_vars_everything')
reticulate::repl_python()
quit
knitr::opts_chunk$set(echo = TRUE)
reticulate::repl_python()
import pandas as pd
mylist = [1, 4, 7, 12]
mylist * 2
quit
plot(pressure)
reticulate::repl_python()
mylist * 2
quit
summary(cars)
reticulate::repl_python()
mylist = [1, 4, 7, 12]
quit
# install from CRAN
install.packages(c('repr', 'IRdisplay',
'evaluate', 'crayon',
'pbdZMQ', 'devtools',
'uuid', 'digest'))
# from Github
devtools::install_github('IRkernel/IRkernel')
install.packages(c("repr", "IRdisplay", "evaluate", "crayon", "pbdZMQ", "devtools", "uuid", "digest"))
install.packages(c("repr", "IRdisplay", "evaluate", "crayon", "pbdZMQ", "devtools", "uuid", "digest"))
install.packages(c("repr", "IRdisplay", "evaluate", "crayon", "pbdZMQ", "devtools", "uuid", "digest"))
install.packages(c("repr", "IRdisplay", "evaluate", "crayon", "pbdZMQ", "devtools", "uuid", "digest"))
install.packages('digest')
install.packages("digest")
install.packages("digest")
library(tidycensus)
library(tidyverse)
puma <-
get_acs(
geography = "public use microdata area",
variable = "B05006_001",
year = 2018,
# wide = TRUE,
geometry = TRUE,
state = st,
keep_geo_vars = TRUE
) %>%
mutate(
sqmile = ALAND10/2589988,
puma_density = estimate/sqmile
) %>%
rename(PUMAID = GEOID)
if (!require(pacman)) install.packages("pacman"); pacman::p_load(googledrive, bit64, fs, data.table, tigris, tidycensus, tidyverse, spdep, raster, sp, parallel, sf)
# use this for Desktop R Studio
# data_dir <- 'C:/Users/madou/OneDrive - UCLA IT Services/1)_PS-Honors/police_killings_github/udp_expansion_matt'
data_dir <- paste0(getwd(), '/udp_expansion_matt')
r_data_folder <- '/data/R_data/'
### Tract data extraction function: add your state here
st <- c("AL", "AK", "AZ", "AR", "CA", "CO", "CT",
"DE", "DC", "FL", "GA", "HI", "ID", "IL",
"IN", "IA", "KS", "KY", "LA", "ME", "MD",
"MA", "MI", "MN", "MS", "MO", "MT", "NE",
"NV", "NH", "NJ", "NM", "NY", "NC", "ND",
"OH", "OK", "OR", "PA", "RI", "SC", "SD",
"TN", "TX", "UT", "VT", "VA", "WA", "WV",
"WI", "WY")
puma <-
get_acs(
geography = "public use microdata area",
variable = "B05006_001",
year = 2018,
# wide = TRUE,
geometry = TRUE,
state = st,
keep_geo_vars = TRUE
) %>%
mutate(
sqmile = ALAND10/2589988,
puma_density = estimate/sqmile
) %>%
rename(PUMAID = GEOID)
save(puma, file = paste0(data_dir, r_data_folder, 'puma.RData')
)
stsf <-
stsp %>%
st_as_sf() %>%
st_transform(4269) %>%
st_centroid() %>%
st_join(., puma) %>%
mutate(dense = case_when(puma_density >= 3000 ~ 1, TRUE ~ 0)) %>%
st_drop_geometry() %>%
select(GEOID, puma_density, dense) %>%
mutate(GEOID = as.numeric(GEOID))
if (!require(pacman)) install.packages("pacman"); pacman::p_load(googledrive, bit64, fs, data.table, tigris, tidycensus, tidyverse, spdep, raster, sp, parallel, sf, foreach, doParallel)
# use this for Desktop R Studio
# data_dir <- 'C:/Users/madou/OneDrive - UCLA IT Services/1)_PS-Honors/police_killings_github/udp_expansion_matt'
data_dir <- paste0(getwd(), '/udp_expansion_matt')
r_data_folder <- '/data/R_data/'
### Tract data extraction function: add your state here
st <- c("AL", "AK", "AZ", "AR", "CA", "CO", "CT",
"DE", "DC", "FL", "GA", "HI", "ID", "IL",
"IN", "IA", "KS", "KY", "LA", "ME", "MD",
"MA", "MI", "MN", "MS", "MO", "MT", "NE",
"NV", "NH", "NJ", "NM", "NY", "NC", "ND",
"OH", "OK", "OR", "PA", "RI", "SC", "SD",
"TN", "TX", "UT", "VT", "VA", "WA", "WV",
"WI", "WY")
stsf <-
stsp %>%
st_as_sf() %>%
st_transform(4269) %>%
st_centroid() %>%
st_join(., puma) %>%
mutate(dense = case_when(puma_density >= 3000 ~ 1, TRUE ~ 0)) %>%
st_drop_geometry() %>%
select(GEOID, puma_density, dense) %>%
mutate(GEOID = as.numeric(GEOID))
# =====================================================
# =====================================================
# DISPLACEMENT TYPOLOGY SET UP
# =====================================================
# =====================================================
if (!require(pacman)) install.packages("pacman"); pacman::p_load(googledrive, bit64, fs, data.table, tigris, tidycensus, tidyverse, spdep, raster, sp, parallel, sf, foreach, doParallel)
# library(sf)
# 2/9/2024: I could not find this package.
# install.packages('colorout')
# options(width = Sys.getenv('COLUMNS'))
# =====================================================
# Pull in data
# =====================================================
# Note: Adjust the cities below if there are additional cities -
# add your city here by importing corresponding database
# you will need to update the 'data_dir' variable to the directory
# you're using
# use this for Desktop R Studio
# data_dir <- 'C:/Users/madou/OneDrive - UCLA IT Services/1)_PS-Honors/police_killings_github/udp_expansion_matt'
data_dir <- paste0(getwd(), '/udp_expansion_matt')
r_data_folder <- '/data/R_data/'
# df <- read_csv(paste0(data_dir, '/data/outputs/databases/zillow_database_2018.csv'))
# save(df, file = paste0(data_dir, r_data_folder, 'zillow_database_2018.RData'))
# load(paste0(data_dir, r_data_folder, 'zillow_database_2018.RData'))
# use this for R Studio Cloud
# data_dir <- '/udp_expansion_matt/data/outputs/databases/'
# df <- read_csv(file = paste0(getwd(), data_dir, 'zillow_database_2018.csv'))
# =====================================================
# Create rent gap and extra local change in rent
# =====================================================
#
# Tract data
# -----------------------------------------------------
# Note: Make sure to extract tracts that surround cities. For example, in
# Memphis and Chicago, TN, MO, MS, and AL are within close proximity of
# Memphis and IN is within close proximity of Chicago.
### Tract data extraction function: add your state here
st <- c("AL", "AK", "AZ", "AR", "CA", "CO", "CT",
"DE", "DC", "FL", "GA", "HI", "ID", "IL",
"IN", "IA", "KS", "KY", "LA", "ME", "MD",
"MA", "MI", "MN", "MS", "MO", "MT", "NE",
"NV", "NH", "NJ", "NM", "NY", "NC", "ND",
"OH", "OK", "OR", "PA", "RI", "SC", "SD",
"TN", "TX", "UT", "VT", "VA", "WA", "WV",
"WI", "WY")
# Wait for above code to complete before running anything more
load(file = 'Feb_13_2024_1_00_PM.RData')
# load(file = 'Feb_13_2024_1_00_PM.RData')
# Subset stsp to remove NAs
stsp_backup <- stsp
stsp <- stsp[!is.na(stsp$tr_pchrent),]
class(stap)
class(stsp)
load("C:/Users/madou/OneDrive - UCLA IT Services/1)_PS-Honors/police-killings-project_union_PC/udp_expansion_matt/data/R_data/stsp.RData")
stsf <-
stsp %>%
st_as_sf() %>%
st_transform(4269) %>%
st_centroid() %>%
st_join(., puma) %>%
mutate(dense = case_when(puma_density >= 3000 ~ 1, TRUE ~ 0)) %>%
st_drop_geometry() %>%
select(GEOID, puma_density, dense) %>%
mutate(GEOID = as.numeric(GEOID))
stsf <-
stsp %>%
st_as_sf() %>%
st_transform(4269) %>%
st_centroid() %>%
st_join(., puma) %>%
mutate(dense = case_when(puma_density >= 3000 ~ 1, TRUE ~ 0)) %>%
st_drop_geometry()
stsf$GEOID <- as.numeric(stsf$GEOID)
stsf <- stsf[, c('GEOID', 'puma_density', 'dense')]
save(stsp, file = paste0(data_dir, r_data_folder, 'stsp.RData'))
reticulate::repl_python()
import sys
import census
quit
library(r-miniconda)
library(rminiconda)
reticulate::repl_python()
import sys
quit
